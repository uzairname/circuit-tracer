{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37d26ac",
   "metadata": {},
   "source": [
    "# Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abfd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "074b75c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec8b5c5e32c4e11ada30bf8b99e637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fd1e89bf23497d8d1274fe75b3d4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\"\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01922ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.19s\n",
      "Found 12392 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 1.13s\n",
      "Phase 2: Building input vectors\n",
      "Selected 3 logits with cumulative probability 0.9766\n",
      "Will include 8192 of 12392 feature nodes\n",
      "Input vectors built in 0.75s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.30s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:08<00:00, 942.68it/s] \n",
      "Feature attributions completed in 8.69s\n",
      "Attribution completed in 11.21s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The war lasted from the year 1711 to 17\"  # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report\n",
    "\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=None,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "graph_dir = 'graphs'\n",
    "graph_name = 'war.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15e01ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12392, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.active_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac81c6",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a4200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "from circuit_tracer.utils.create_graph_files import load_graph_data\n",
    "\n",
    "\n",
    "graph_dir = 'graphs'\n",
    "graph_name = 'war.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph = load_graph_data(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c369f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning graph\n",
      "Nodes kept: 99.54%\n",
      "Edges kept: 100.00%\n",
      "Nodes kept: 99.54%\n",
      "Edges kept: 100.00%\n",
      "\n",
      "Original graph scores: (0.7420152425765991, 0.9321903586387634)\n",
      "\n",
      "Original graph scores: (0.7420152425765991, 0.9321903586387634)\n",
      "Pruned graph scores:   (0.7420152425765991, 0.9321903586387634)\n",
      "Pruned graph scores:   (0.7420152425765991, 0.9321903586387634)\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.graph import compute_graph_scores, compute_graph_scores_masked, prune_graph\n",
    "\n",
    "node_threshold=1\n",
    "edge_threshold=1\n",
    "\n",
    "node_mask, edge_mask, cumulative_scores = prune_graph(graph, node_threshold, edge_threshold)\n",
    "\n",
    "# Sparsity stats\n",
    "print(f\"Nodes kept: {node_mask.sum().item() / len(node_mask):.2%}\")\n",
    "print(f\"Edges kept: {edge_mask.sum().item() / edge_mask.numel():.2%}\")\n",
    "\n",
    "# Compare scores: original vs pruned\n",
    "print(f\"\\nOriginal graph scores: {compute_graph_scores(graph)}\")\n",
    "print(f\"Pruned graph scores:   {compute_graph_scores_masked(graph, node_mask, edge_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ddb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.7420152425765991, 0.9321903586387634)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5741bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6359, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.active_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12610e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276.5351556567957"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9225512528473804 0.19385499706253542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.7179655432701111, 0.925042450428009)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e057db",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"dallas-austin\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "# pruning step\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac413f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
